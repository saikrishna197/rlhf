{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab573a1-cdb9-4153-870f-d798fca93dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fb6c17c592459ebb16521a19f0470c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()  # same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07d1a89-78b3-49f3-a6d7-6b6c9a36f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e9e2e3-8313-4cbc-9713-0d4cc5d1fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        text = prepare_sample_text(example)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "\n",
    "    return total_characters / total_tokens\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe58e8e3-263b-4f8d-be4a-2bb1ac418c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/sai\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfc136-769a-4ed9-9ae5-80eb1547e25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7abde9-3176-457b-95c0-944faabaa5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7900757-d4f1-476f-8c60-2fce336eab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set: 229. Size of the validation set: 2\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('json',data_files='mungem.jsonl',split = \"train\",use_auth_token=True)\n",
    "data = data.train_test_split(test_size=0.005, seed=None)\n",
    "train_data = data[\"train\"]\n",
    "valid_data = data[\"test\"]\n",
    "print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9224c20-d931-4a61-b7bc-376d9e2a3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample_text(example):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    text = f\"{example['prompt']}\\n {example['response']}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_datasets(tokenizer):\n",
    "    dataset = load_dataset('json',data_files='mungem.jsonl',split = \"train\",use_auth_token=True)\n",
    "    dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
    "    train_data = dataset[\"train\"]\n",
    "    valid_data = dataset[\"test\"]\n",
    "    print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n",
    "\n",
    "    chars_per_token = chars_token_ratio(train_data, tokenizer)\n",
    "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
    "\n",
    "    train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        formatting_func=prepare_sample_text,\n",
    "        infinite=True,\n",
    "        seq_length=256,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    valid_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        formatting_func=prepare_sample_text,\n",
    "        infinite=False,\n",
    "        seq_length=256,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1efc4-1d47-4964-b373-b8e48cb8d550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22e658-0ee8-43a4-a7e9-3d7fa0a0b0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ed5893-8cfd-443d-8750-1ebc7690418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f780a03772b4512977c0ce7d66bd44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"PharMolix/BioMedGPT-LM-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aafe93c-c14b-44d3-bb71-482613bd4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=True' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ca358d61f44dfa99e74226216c50ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d431f642e4974cca8dd52e3a0dc62155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c186862aea4a3185f81ca7c7dd60c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set: 229. Size of the validation set: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2084 > 2048). Running this sequence through the model will result in indexing errors\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 229/400 [00:00<00:00, 401.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character to token ratio of the dataset is: 2.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/trl/trainer/utils.py:465: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictionary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"PharMolix/BioMedGPT-LM-7B\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sft_biomedgpt7b\",\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=10,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=105,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    run_name=\"sft_biomedgpt7b\",\n",
    ")\n",
    "\n",
    "train_dataset, eval_dataset = create_datasets(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ac6260-c7c7-44fb-8071-31f1bf82ad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:227: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 50:11, Epoch 17/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.651800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.650300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/trl/trainer/utils.py:487: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/trl/trainer/utils.py:487: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdf5d9b138f452bb4092b9192d731d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/ppo/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:508: UserWarning: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration. This warning will be raised to an exception in v4.34.\n",
      "\n",
      "Thrown during validation:\n",
      "`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    packing=True,\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"sft_biomedgpt7b\")\n",
    "\n",
    "output_dir = os.path.join(\"sft_biomedgpt7b\", \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "# Free memory for merging weights\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = os.path.join(\"sft_biomedgpt7b\", \"final_merged_checkpoint\")\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e93624-3942-46da-b62f-7f14cfe2b9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3dc49-1ea8-4cf5-95fb-3f0825d8ce8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c9bee-b242-42eb-95aa-45a1395a339a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
